# Day 4. 학습 사이클과 데이터

이 장에서는 **모델이 데이터를 공부하는 구체적인 방법(학습 사이클)**과 **목적에 따른 데이터 관리법(데이터 분리)**을 다룬다.

복잡한 수식 대신, 우리가 학창 시절 겪었던 **'시험 공부'** 과정에 빗대어 설명한다.

---

## 1. 전체 그림: 데이터 농사 짓기



우리가 가진 **Dataset(전체 데이터)**은 농사를 짓기 위한 씨앗과 같다. 이 씨앗을 모두 한 번에 심지 않고, 용도에 따라 세 가지 바구니에 나누어 담는다.

### 데이터 3단 분리
1. **Training Data (공부용)**: 실력을 키우는 단계
2. **Validation Data (점검용)**: 잘 배우고 있는지 확인하는 단계
3. **Test Data (평가용)**: 최종 실력을 검증하는 단계

### 학습 사이클 요약
1. Training Data를 작은 묶음(**Mini-batch**)으로 나눈다.
2. 한 묶음씩 풀고 오답 노트(**Loss**)를 확인하며 실력을 고친다.
3. 전체 문제집을 한 번 다 풀면 **1 Epoch**가 끝난다.
4. 이 과정을 여러 번 반복한다.

---

## 2. 데이터의 역할: 공부, 모의고사, 수능

데이터를 나누지 않고 몽땅 학습에 쓰면, 모델은 답을 **'이해'**하는 게 아니라 **'암기'**해버린다. 이를 방지하기 위해 데이터를 철저히 분리한다.

| 구분 | 영어 명칭 | 역할 비유 | 사용 시점 | 권장 비율(예시) |
| :--- | :--- | :--- | :--- | :--- |
| **학습 데이터** | Training Data | **문제집 (개념 원리)** | 모델이 직접 공부할 때 | 60 ~ 80% |
| **검증 데이터** | Validation Data | **모의고사** | 학습 중간에 실력 점검 | 10 ~ 20% |
| **테스트 데이터**| Test Data | **수능 (최종 시험)** | 모든 학습이 끝난 후 | 10 ~ 20% |

> **💡 왜 검증(Validation)이 따로 필요한가요?**
>
> 훈련 데이터만 계속 보면 모델이 문제와 정답을 통째로 외워버리는 **과적합(Overfitting)** 상태가 됩니다.
> "이 문제는 답이 3번이야"라고 외운 상태인지, 진짜 풀 줄 아는 상태인지 확인하기 위해 **"안 풀어본 문제(모의고사)"**로 중간 점검을 해야 합니다.

---

## 3. 학습의 단위: Epoch, Batch, Iteration

딥러닝 학습 과정을 이해하려면 이 세 가지 숫자의 관계를 꼭 알아야 한다.

### 1) Epoch (에포크)
- **정의**: 문제집(Training Data) **전체를 한 번 끝까지 훓어보는 것**.
- **비유**: "수학의 정석 **1회독, 2회독, 3회독**" 할 때의 그 '회독' 수.
- **특징**: 1 Epoch가 끝날 때마다 데이터를 뒤섞어(**Shuffle**) 순서를 바꾼다. (문제 순서를 외우지 못하게 하기 위함)

### 2) Batch Size (배치 크기)
- **정의**: 모델이 한 번에 학습할 **데이터의 개수**.
- **비유**: "오늘 영어 단어 **32개씩** 끊어서 외울 거야."
- **특징**:
  - 크기가 크면: 속도는 빠르지만, 컴퓨터 메모리가 많이 필요하다.
  - 크기가 작으면: 세밀하게 학습하지만, 시간이 오래 걸린다.

### 3) Iteration (이터레이션 / 스텝)
- **정의**: 1 Epoch를 마치기 위해 **필요한 반복 횟수**. (배치를 몇 번 가져와야 하는가?)
- **비유**: 문제집이 100쪽이고 하루에 10쪽씩 푼다면, 1회독을 위해 **10일(10번)**이 걸린다.

---

## 4. 숫자로 보는 학습 사이클 (예시)

가장 헷갈리기 쉬운 부분이다. 아래 예시를 통해 구조를 확실히 잡아보자.

**[상황 설정]**
- **전체 데이터**: 10,000개
- **배치 크기(Batch Size)**: 100개

**[계산 결과]**
1. **Mini-batch 1개**: 데이터 100개 묶음
2. **1 Iteration**: 100개 묶음 하나를 학습하는 과정
3. **1 Epoch를 위한 반복 횟수**:
   > 전체 10,000개 ÷ 100개씩 묶음 = **100번 (100 Iterations)**

**[결론]**
- 모델은 **100번**의 학습(수정) 과정을 거쳐야 **1 Epoch(1회독)**를 완료한다.
- 만약 **Epoch = 5**로 설정했다면?
  > 100번 반복 × 5회독 = **총 500번**의 학습(업데이트)이 일어난다.



---

## 5. 핵심 요약 (Cheat Sheet)

이것만은 꼭 기억하자!

- **Dataset**: 전체 재료. (Train / Val / Test로 나뉨)
- **Training Data**: 실력을 키우는 **문제집**.
- **Validation Data**: 과적합(암기)을 막는 **모의고사**.
- **Test Data**: 최종 성능을 검증하는 **수능**.
- **Epoch**: 문제집 전체를 **1회독** 하는 것.
- **Batch Size**: 한 번에 공부할 **문제의 양**.
- **Iteration**: 1회독을 위해 반복해야 하는 **횟수**.

---
