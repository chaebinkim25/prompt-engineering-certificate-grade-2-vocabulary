# Day 3. 학습의 수학적 원리

3일 차에 오신 것을 환영합니다! 
오늘은 인공지능 학습의 심장이라고 할 수 있는 수학적 원리를 다룹니다.

복잡한 수식이 나오면 당황스러울 수 있지만, 
사실 이 모든 과정은 "틀린 정도를 측정하고(Loss), 
줄이는 방향을 찾아(Gradient), 조금씩 고쳐나가는(Update)" 
것의 반복일 뿐입니다. 
**안개 낀 산에서 내려오는 등산객**에 비유하여 하나씩 봅시다.

---

# 1) Loss Function (손실 함수): "얼마나 틀렸는가?"
모델이 예측을 내놓았을 때, 
정답과 얼마나 차이가 나는지 점수로 매기는 단계입니다. 
비용(Cost) 또는 **손실(Loss)**이라고 부릅니다.

## 산에서 내려오는 등산객 비유
- 산 정상(오차 최대)에 있는지, 골짜기(오차 최소)에 있는지 확인하는 고도계입니다.
- Loss(오차)가 **작으면** 잘한 것  
- Loss(오차)가 **크면** 못한 것

## 손실 함수(Loss Function)란?
- 손실을 계산하는 **규칙(공식)** 입니다.
- 학습의 목표는 단순합니다:

> **손실을 최소로 만드는 것**

## 왜 손실을 숫자로 만들까?
컴퓨터는 “더 잘했네/못했네” 같은 말보다 **숫자**가 있어야
- 비교할 수 있고
- 줄이는 방향으로 개선할 수 있습니다.

## 손실 함수 (Loss function)의 종류
> 정답이 숫자로 나오는 경우: 평균제곱오차(Mean Squared Error)
> 정답이 단어로 나오는 경우: 교차 엔트로피 손실(Cross-Entrop Loss)
---

# 2) Mean Squared Error (평균제곱오차, MSE)

여러 데이터에서 결과가 벗어난 정도를 숫자로 표현할 수 있을 때 쓰는 방법이다.
# 비유: 양궁에서 화살이 중앙에서 평균적으로 얼마나 빗나갔는지 측정하기

## 오차 (Error): "얼마나 빗나갔니?"
- 첫 번째 화살: 정중앙에서 **1cm** 빗나감. (아깝다!)
- 두 번째 화살: 정중앙에서 **10cm** 빗나감. (망했다!)

이 '빗나간 거리'가 수학에서 말하는 '오차(결과값 - 목표값)'이다. 

## 제곱 (Square): "큰 실수에는 가혹한 벌을!" (핵심⭐)
여기가 MSE의 **가장 중요한 특징**이다. 
점수를 매기는 코치님은 아주 엄격해서, 그냥 빗나간 거리만큼 벌점을 주는게 아니라
**"거리를 제곱해서"** 벌점을 준다.
- 1cm 빗나간 학생: $1 \times 1 = \mathbf{1}$점 감점. (이 정도는 봐줌)
- 10cm 빗나간 학생: $10 \times 10 = \mathbf{100}$점 감점. (초비상!)

**🤔 코치님은 왜 이렇게 '제곱'을 해서 불공평하게 점수를 매길까**
- **크기 구하기**: 위쪽(+)으로 빗나가든, 아래쪽(-)으로 빗나가든, 제곱하면 전부 양수가 되서 계산하기 편하다.
- **큰 실수 박멸**: 자잘하게 1cm 틀리는 건 참아주겠지만, **10cm씩 왕창 틀리는 건 절대 용납 못해!"** 라는 뜻이다. 실수가 10배 커지면 벌점은 100배 커지니까, 무조건 큰 실수를 고치게 된다.

## 평균 (Mean): "그래서 내 평균 실력은?"
화살을 딱 한 발만 쏘고 실력을 평가할 순 없다. 
100발 쐈다면 그 벌점들을 다 더해서 100으로 나눈다. 그것이 **평균(Mean)**이다.

✅ **3줄 요약 (MSE = 엄격한 코치님)**  

MSE 공식($\frac{1}{N}\sum(y-\hat{y})^2$)을 글자로 풀면 이렇다.

1) E (Error, 오차): "얼마나 빗나갔는지 재고,"
2) S (Square, 제곱): "많이 틀릴수록 벌점을 뻥튀기해서 매긴 다음,"
3) M (Mean, 평균): "그 벌점들의 평균을 낸다."

---


# 3) Cross-Entropy Loss (교차 엔트로피 손실)

주로 **분류(Classification)** 문제에서 사용합니다.  

  **분류** = 결과가 “고양이/강아지/토끼”처럼 **종류(카테고리)** 중 하나로 나오는 문제

MSE가 "숫자 맞추기"용이라면, Cross-Entropy는 **"정답 고르기(분류)"** 전용 채점표입니다.

---

## 핵심 아이디어: "확률의 차이"
분류 문제에서 AI는 "이건 고양이야!"라고 단정짓지 않습니다. 
대신 **확률**로 대답합니다.

- "고양이일 확률 80%, 강아지일 확률 20%입니다."

여기서 **Cross-Entropy**는 AI가 **정답에 얼마나 높은 확신(확률)을 가졌는지**를 보고 점수를 매깁니다.

## 작동 원리: "자신만만하게 틀리면 박살을 낸다"
이 손실 함수의 성격을 가장 잘 설명하는 단어는 **'거만함에 대한 처벌'**입니다.

예를 들어, **정답이 '고양이'**인 사진이 있다고 해봅시다. 
두 명의 AI 학생(A, B)가 대답합니다.

- **학생 A**: "음... 잘 모르겠는데, **고양이일 확률 60%** 정도?"
  - 정답을 맞췄지만 확신이 부족했죠? $\rightarrow$ **약한 벌점 (Loss 0.5)**
- **학생 B**: "이건 무조건 강아지야! **강아지일 확률 99%!** (고양이일 확률 1%)"  
  - 정답(고양이)를 **매우 낮은 확률**로 예측했죠? 즉, **틀린 답을 자신만만하게 외쳤습니다.**
  - $\rightarrow$ **어마무시한 벌점 (Loss 4.6)**

## 가장 단순한 형태(정답이 하나일 때)
정답 클래스에 모델이 준 확률을 `p`라고 하면:

\[
\text{Cross-Entropy Loss} = -\log(p)
\]

복잡해보이지만, 핵심은 정답확률이 낮을수록 손실이 무한대로 증가한다는 것입니다.

✅ **정리:**  
분류에서는 “정답에 준 확률”을 중심으로 벌점을 주는 **교차 엔트로피**가 잘 맞습니다.

---


# 4) Gradient Descent (경사하강법)

손실을 줄이려면 모델 내부의 설정값을 바꿔야 합니다.  
그 “바꾸는 규칙”이 **경사하강법**입니다.

## 양궁 선수 비유
양궁 선수가 **어깨 각도($w_1$)** 와 **팔꿈치 각도($w_2$)**, 두 관절을 모두 조절해서 활을 쏘고 있습니다.

**Forward (예측): "일단 쏴봐!"**
- 현재 상태:
  - 어깨 각도 ($w_1$): 30도
  - 팔꿈치 각도 ($w_2$): 90도
- **결과:** 슝~ 퍽! 정중앙에서 **50cm 빗나갔습니다.

**Gradient (원인 분석): "뭐가 문제야?" (민감도 체크)**
이 오차에 대해 각 관절이 **얼마나 책임이 있는지** 따져봅니다. 
- 머리속으로 시뮬레이션해본 결과:
  - 어깨 각도($w_1$)를 1도 증가했을 때 결과 변화: 10cm 더 빗나간다 (책임 큼)
  - 팔꿈치 각도($w_2$)를 1도 증가했을 때 결과 변화: 1cm 가까워진다 (책임 작음)

**Update (수정): "책임만큼 고쳐라"**
책임(기울기)가 큰 녀석은 많이, 작은 녀석은 조금 고칩니다. 
- 어깨: 책임이 크니까 **1도** 확 낮춤
- 팔꿈치: 책임이 작으니까 **0.1도** 살짝 높임

## 전문용어 정리(쉬운 뜻)
- **파라미터(Parameter)**: 모델 안의 “조절 다이얼 값” (예: 각 입력의 중요도, 출력시 정해진 기준)
- **기울기(Gradient)**: “조금 움직였을 때 손실이 얼마나 변하는지”를 나타내는 값  
- **학습률(Learning Rate)**: 한 번에 **얼마나 많이** 파라미터를 바꿀지 정하는 숫자

## 한줄 요약
- Gradient Descent (경사하강법)은 뉴런 하나에서 출력의 손실을 최소화하기 위해 각 파라미터를 얼마나 바꿀지를 계산하는 방법입니다.
---

# 5) Backpropagation (역전파)

**경사하강법**은 “각 뉴런이 손실에 기여하는 정도를 알고 있을 때, 파라미터를 어떻게 바꿀지(업데이트 규칙)”이고,  
**역전파**는 “다층신경망에서 각 뉴런이 최종 손실에 기여하는 정도를 계산하는 방법”입니다.

## 역전파의 원리
가장 말단에 있는 뉴런들은 출력에 연결되어 있으니 최종 손실에 그대로 기여합니다.

말단의 한칸 전에 있는 뉴런들의 기여도는 말단에 있는 뉴런에서 설정된 중요도에 따라 결정됩니다. 

말단의 두칸 전에 있는 뉴런들의 기여도는 말단의 한칸 전에 있는 뉴런들에서 설정된 중요도와 한칸 전 뉴런들의 기여도에 따라 결정됩니다.

같은 방식으로 모든 뉴런들의 기여도를 결정하고, 경사 하강법을 통해 파라미터를 수정할 수 있습니다.

---

# 용어 미니사전(헷갈릴 때 보기)

- **분류(Classification)**: 여러 “종류” 중 하나를 맞히기  
- **회귀(Regression)**: 연속적인 “숫자 값”을 맞히기  
- **파라미터(Parameter)**: 모델의 내부 조절값(다이얼)  
- **학습률(Learning Rate)**: 한 번에 움직이는 크기  
- **기울기(Gradient)**: 파라미터를 조금 바꿨을 때 손실이 얼마나 변하는지  
- **연쇄법칙(Chain Rule)**: 여러 단계 함수에서 영향 전달을 계산하는 규칙
