# Day 3. 학습의 수학적 원리

3일 차에 오신 것을 환영합니다! 
오늘은 인공지능 학습의 심장이라고 할 수 있는 수학적 원리를 다룹니다.

복잡한 수식이 나오면 당황스러울 수 있지만, 
사실 이 모든 과정은 "틀린 정도를 측정하고(Loss), 
줄이는 방향을 찾아(Gradient), 조금씩 고쳐나가는(Update)" 
것의 반복일 뿐입니다. 
**안개 낀 산에서 내려오는 등산객**에 비유하여 하나씩 봅시다.

---

# 1) Loss Function (손실 함수): "얼마나 틀렸는가?"
모델이 예측을 내놓았을 때, 
정답과 얼마나 차이가 나는지 점수로 매기는 단계입니다. 
비용(Cost) 또는 **손실(Loss)**이라고 부릅니다.

## 산에서 내려오는 등산객 비유
- 산 정상(오차 최대)에 있는지, 골짜기(오차 최소)에 있는지 확인하는 고도계입니다.
- Loss(오차)가 **작으면** 잘한 것  
- Loss(오차)가 **크면** 못한 것

## 손실 함수(Loss Function)란?
- 손실을 계산하는 **규칙(공식)** 입니다.
- 학습의 목표는 단순합니다:

> **손실을 최소로 만드는 것**

## 왜 손실을 숫자로 만들까?
컴퓨터는 “더 잘했네/못했네” 같은 말보다 **숫자**가 있어야
- 비교할 수 있고
- 줄이는 방향으로 개선할 수 있습니다.

## 손실 함수 (Loss function)의 종류
> 정답이 숫자로 나오는 경우: 평균제곱오차(Mean Squared Error)
> 정답이 단어로 나오는 경우: 교차 엔트로피 손실(Cross-Entrop Loss)
---

# 2) Mean Squared Error (평균제곱오차, MSE)

여러 데이터에서 결과가 벗어난 정도를 숫자로 표현할 수 있을 때 쓰는 방법이다.
# 비유: 양궁에서 화살이 중앙에서 평균적으로 얼마나 빗나갔는지 측정하기

## 오차 (Error): "얼마나 빗나갔니?"
- 첫 번째 화살: 정중앙에서 **1cm** 빗나감. (아깝다!)
- 두 번째 화살: 정중앙에서 **10cm** 빗나감. (망했다!)

이 '빗나간 거리'가 수학에서 말하는 '오차(결과값 - 목표값)'이다. 

## 제곱 (Square): "큰 실수에는 가혹한 벌을!" (핵심⭐)
여기가 MSE의 **가장 중요한 특징**이다. 
점수를 매기는 코치님은 아주 엄격해서, 그냥 빗나간 거리만큼 벌점을 주는게 아니라
**"거리를 제곱해서"** 벌점을 준다.
- 1cm 빗나간 학생: $1 \times 1 = \mathbf{1}$점 감점. (이 정도는 봐줌)
- 10cm 빗나간 학생: $10 \times 10 = \mathbf{100}$점 감점. (초비상!)

**🤔 코치님은 왜 이렇게 '제곱'을 해서 불공평하게 점수를 매길까**
- **크기 구하기**: 위쪽(+)으로 빗나가든, 아래쪽(-)으로 빗나가든, 제곱하면 전부 양수가 되서 계산하기 편하다.
- **큰 실수 박멸**: 자잘하게 1cm 틀리는 건 참아주겠지만, **10cm씩 왕창 틀리는 건 절대 용납 못해!"** 라는 뜻이다. 실수가 10배 커지면 벌점은 100배 커지니까, 무조건 큰 실수를 고치게 된다.

## 평균 (Mean): "그래서 내 평균 실력은?"
화살을 딱 한 발만 쏘고 실력을 평가할 순 없다. 
100발 쐈다면 그 벌점들을 다 더해서 100으로 나눈다. 그것이 **평균(Mean)**이다.

✅ **3줄 요약 (MSE = 엄격한 코치님)**  

MSE 공식($\frac{1}{N}\sum(y-\hat{y})^2$)을 글자로 풀면 이렇다.

1) E (Error, 오차): "얼마나 빗나갔는지 재고,"
2) S (Square, 제곱): "많이 틀릴수록 벌점을 뻥튀기해서 매긴 다음,"
3) M (Mean, 평균): "그 벌점들의 평균을 낸다."

---


# 3) Cross-Entropy Loss (교차 엔트로피 손실)

주로 **분류(Classification)** 문제에서 사용합니다.  

  **분류** = 결과가 “고양이/강아지/토끼”처럼 **종류(카테고리)** 중 하나로 나오는 문제

MSE가 "숫자 맞추기"용이라면, Cross-Entropy는 **"정답 고르기(분류)"** 전용 채점표입니다.

---

## 핵심 아이디어: "확률의 차이"
분류 문제에서 AI는 "이건 고양이야!"라고 단정짓지 않습니다. 
대신 **확률**로 대답합니다.

- "고양이일 확률 80%, 강아지일 확률 20%입니다."

여기서 **Cross-Entropy**는 AI가 **정답에 얼마나 높은 확신(확률)을 가졌는지**를 보고 점수를 매깁니다.

## 작동 원리: "자신만만하게 틀리면 박살을 낸다"
이 손실 함수의 성격을 가장 잘 설명하는 단어는 **'거만함에 대한 처벌'**입니다.

예를 들어, **정답이 '고양이'**인 사진이 있다고 해봅시다. 
두 명의 AI 학생(A, B)가 대답합니다.

- **학생 A**: "음... 잘 모르겠는데, **고양이일 확률 60%** 정도?"
  - 정답을 맞췄지만 확신이 부족했죠? $\rightarrow$ **약한 벌점 (Loss 0.5)**
- **학생 B**: "이건 무조건 강아지야! **강아지일 확률 99%!** (고양이일 확률 1%)"  
  - 정답(고양이)를 **매우 낮은 확률**로 예측했죠? 즉, **틀린 답을 자신만만하게 외쳤습니다.**
  - $\rightarrow$ **어마무시한 벌점 (Loss 4.6)**

## 가장 단순한 형태(정답이 하나일 때)
정답 클래스에 모델이 준 확률을 `p`라고 하면:

\[
\text{Cross-Entropy Loss} = -\log(p)
\]

복잡해보이지만, 핵심은 정답확률이 낮을수록 손실이 무한대로 증가한다는 것입니다.

✅ **정리:**  
분류에서는 “정답에 준 확률”을 중심으로 벌점을 주는 **교차 엔트로피**가 잘 맞습니다.

---


# 4) Gradient Descent (경사하강법)

손실을 줄이려면 모델 내부의 설정값을 바꿔야 합니다.  
그 “바꾸는 규칙”이 **경사하강법**입니다.

## 양궁 선수 비유
양궁 선수가 **어깨 각도($w_1$)** 와 **팔꿈치 각도($w_2$)**, 두 관절을 모두 조절해서 활을 쏘고 있습니다.

**Forward (예측): "일단 쏴봐!"**
- 현재 상태:
  - 어깨 각도 ($w_1$): 30도
  - 팔꿈치 각도 ($w_2$): 90도
- **결과:** 슝~ 퍽! 정중앙에서 **위로 

## 전문용어 정리(쉬운 뜻)
- **파라미터(Parameter)**: 모델 안의 “조절 다이얼 값” (예: 가중치, 편향 같은 값들)
- **기울기(Gradient)**: “조금 움직였을 때 손실이 얼마나 변하는지”를 나타내는 값  
  - 한 값이면 기울기, 여러 값이면 기울기들의 묶음(방향)이라고 생각하면 됩니다.
- **학습률(Learning Rate)**: 한 번에 **얼마나 크게** 움직일지 정하는 숫자

## 직관: 산 내려가기
- 손실을 “산의 높이”라고 보면,
- 우리는 가장 낮은 곳(손실 최소)으로 내려가고 싶습니다.
- 기울기(gradient)는 “가장 가파르게 올라가는 방향”을 알려주므로,
- 그 반대 방향으로 움직이면 손실이 줄어듭니다.

## 업데이트 규칙(가장 기본 형태)
\[
w \leftarrow w - \alpha \frac{\partial L}{\partial w}
\]

- `w`: 파라미터  
- `L`: 손실  
- `\frac{\partial L}{\partial w}`: “w를 조금 바꾸면 손실이 얼마나 바뀌는지”
- `\alpha`: 학습률(움직이는 크기)

## 초간단 예시(한 번 업데이트)
\[
L(w) = (w - 3)^2
\]
\[
\frac{dL}{dw} = 2(w-3
\]

초기 `w = 0`, 학습률 `\alpha = 0.1`

- 기울기: `2(0-3) = -6`
- 업데이트:
  \[
  w \leftarrow 0 - 0.1 \times (-6) = 0.6
  \]

➡️ `w`가 3에 가까워지는 방향으로 움직였습니다.

---

# 5) Backpropagation (역전파)

**경사하강법**은 “어떻게 바꿀지(업데이트 규칙)”이고,  
**역전파**는 “바꾸는 데 필요한 기울기(gradient)를 빠르게 계산하는 방법”입니다.

## 왜 역전파가 필요할까?
신경망(딥러닝)은 계산이 여러 단계로 이어집니다.

- 입력 → (여러 단계 계산) → 예측 → 손실

각 단계의 파라미터가 손실에 얼마나 영향을 줬는지(=기울기)를 알아야 업데이트할 수 있는데,  
이를 매번 하나하나 직접 계산하면 너무 느립니다.

## 핵심 원리: 연쇄법칙(Chain Rule)
- **연쇄법칙(Chain Rule)**: 함수가 여러 겹으로 이어질 때, 영향(변화)이 어떻게 전달되는지 계산하는 규칙

예를 들어
\[
y = f(g(x))
\]
이면
\[
\frac{dy}{dx} = \frac{dy}{dg}\cdot\frac{dg}{dx}
\]

즉, “x가 y에 미치는 영향”을
- “x가 중간값에 미치는 영향”
- “중간값이 y에 미치는 영향”
을 곱해서 구합니다.

## 역전파의 동작(흐름)
1. **순전파(Forward pass)**: 입력 → 예측 → 손실 계산  
2. **역전파(Backward pass)**: 손실에서부터 거꾸로 내려오며  
   각 파라미터의 기울기를 연쇄법칙으로 효율적으로 계산

✅ **정리:**  
- 경사하강법 = “업데이트(파라미터를 어떻게 바꿀지)”  
- 역전파 = “기울기 계산(얼마나 바꿀지 계산)”  

---

# 전체 흐름 한 장 요약(암기용)

1. **손실 함수(Loss Function)**로 “틀림 점수”를 만든다  
2. 문제에 맞는 손실을 고른다  
   - 분류: **교차 엔트로피**
   - 숫자 예측: **MSE**
3. 손실을 줄이기 위해 **경사하강법**으로 파라미터를 업데이트한다  
4. 업데이트에 필요한 기울기는 **역전파**로 계산한다  

---

# 연습문제(짧게)

1) 분류: 정답 클래스 확률이 `0.8`이면 교차 엔트로피 손실은 대략?  
- 힌트: \(-\log(0.8)\) (자연로그 기준 약 `0.223`)

2) 회귀: 정답 `5`, 예측 `9`일 때 한 데이터의 제곱오차는?  
- \((9-5)^2 = 16\)
양
3) 경사하강: \(L(w)=(w-10)^2\), 초기 \(w=0\), 학습률 0.1이면 한 번 업데이트 후 \(w\)는?  
- 기울기 \(2(w-10)=-20\) → \(w = 0 - 0.1(-20)=2\)

---

# 용어 미니사전(헷갈릴 때 보기)

- **분류(Classification)**: 여러 “종류” 중 하나를 맞히기  
- **회귀(Regression)**: 연속적인 “숫자 값”을 맞히기  
- **파라미터(Parameter)**: 모델의 내부 조절값(다이얼)  
- **학습률(Learning Rate)**: 한 번에 움직이는 크기  
- **기울기(Gradient)**: 파라미터를 조금 바꿨을 때 손실이 얼마나 변하는지  
- **연쇄법칙(Chain Rule)**: 여러 단계 함수에서 영향 전달을 계산하는 규칙
