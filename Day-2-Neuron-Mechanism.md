# 📘 Day 2. 뉴런의 작동 원리

> **목표**  
> 현대 딥러닝의 가장 기본 단위인 뉴런이 초기 딥러닝에서 쓰던 퍼셉트론보다 더 발전시켜준 
> **활성화 함수**의 역할을 이해한다.  

---

## 1️⃣ Neuron (뉴런)

퍼셉트론은 뇌세포를 본따서 1950년도에 개발된 수학적 모델이고, 
(현대의 인공) 뉴런은 퍼셉트론을 개량해서 지금의 딥러닝에서 쓰는 기본 단위다. 

퍼셉트론과 뉴런의 차이점은 출력을 만드는 방식이다. 

퍼셉트론은 입력된 정보를 모은 결과가 정해진 기준을 초과하면 1, 기준 이하면 0으로 출력 값이 두개만 있었지만, 

뉴런에서는 결과를 정해진 구간 안의 숫자로 내보낸다. 

예를 들면, 오늘 우산을 챙길지 말지 결정해주는 역할. 

입력 1: 비가 온다 (중요도 높음)

입력 2: 바림이 분다 (중요도 낮음)

퍼셉트론 결과: (비 * 중요도) + (바람 * 중요도) > 기준치일때, 우산을 챙긴다!

뉴런의 결과: (비 * 중요도) + (바람 * 중요도)로부터 우산을 챙겨야 하는 것이 얼마나 확실한지 점수를 계산해서 알려준다.

입력값의 합에서 출력을 계산하는 방법을 활성화 함수라고 한다. 

---

## 2️⃣ Activation Function (활성화 함수)


### 한 문장 요약
**활성화 함수는 의미 없는 정보를 차단하고, 의미 있는 정보는 정도를 보존해준다.**

### 왜 필요한가?
- 퍼셉트론은 모든 입력 소스에 대해 중요도를 적절하게 정해주면 어떤 결과를 만들어낼 수는 있었지만, 결과가 틀렸을 때, 틀렸다는 것만 알고, 얼마나 틀렸는지는 알 수 없어서 학습에는 어려움이 있었다.
- 활성화 함수를 쓴 다음부터는 얼마나 틀렸는지 정보가 있기 때문에, 이를 활용해서 입력 소스들에 대한 중요도를 얼마나 바꿀지 정할 수 있게 되어 학습이 가능해졌다.

### 활성화 함수의 역할
- 의미 없는 값 → 줄이거나 버림  
- 의미 있는 값 → 강조해서 전달  

📌 활성화 함수가 없으면  
→ 딥러닝은 **절대 학습할 수 없다**

대표적인 활성화 함수로 Sigmoid (시그모이드), ReLu (렐루), Tanh (탄에이치), Softmax(소프트맥스)가 있다.

---

## 3️⃣ Sigmoid (시그모이드)


### 한 문장 요약
**활성화 함수 중에 뉴런을 퍼셉트론과 가장 비슷하게 동작시키는 함수다**

### 직관적인 이해
- 임계값이 들어오면 0.5를 출력한다.
- 값이 커질수록 1에 가까운 값을 출력한다.
- 값이 작아질수록 0에 가까운 값을 출력한다.

### 사용 예
- 우산 챙기기에서, 비와 바람의 중요도를 계산해서 합친 점수가 0점 이상이면 우산을 챙기는 것이 합리적이라고 가정.
- 점수가 5점인 경우, 50%라고 애매하게 말한다.
- 점수가 10점인 경우, 99%로 권장된다고 말한다.
- 점수가 0점인 경우, 1%로 권장된다고 말한다.

📌 단점
- 값이 너무 크거나 작아지면  
  → 변화가 거의 멈춤  
  → 학습이 느려진다

---

## 4️⃣ ReLU (렐루)


### 한 문장 요약
**ReLU는 현재 딥려닝에서 가장 많이 사용하는 함수다.**

### 규칙을 말로 쓰면
- 0 이하의 값이 들어오면 0을 출력한다. 
- 0보다 큰 값이 들어오면 들어온 값을 그대로 출력한다.
 
### 사용 예
- 우산 챙기기에서, 비와 바람의 중요도를 계산해서 합친 점수가 5점 이상이면 우산을 챙기는 것이 합리적이라고 가정.
- 점수가 10점인 경우, 10만큼 권장된다고 말한다.
- 점수가 5점보다 작은 경우, 출력을 0으로 만들어서 아예 신경을 끄게 만든다. 

### 왜 많이 쓰일까?
- 계산이 매우 빠르다  
- 학습이 잘 된다  
- 구조가 단순하다  

📌 **현재 딥러닝의 기본 선택**

⚠️ 단점
- 어떤 뉴런은 계속 0만 나와서  
  → 완전히 역할을 못 하게 될 수도 있다

---

## 5️⃣ Tanh (탄에이치)


### 한 문장 요약
**Tanh는 호불호가 확실한 비평가 함수다.**

### 시그모이드와 비슷하지만, 출력 범위에서 차이가 난다
- Sigmoid → 0 ~ 1  
- Tanh → **-1 ~ 1**

### 사용 예
- 우산 챙기기에서, 비와 바람의 중요도를 계산해서 합친 점수가 5점 이상이면 우산을 챙기는 것이 합리적이라고 가정.
- 점수가 10점인 경우, +1로 적극 찬성한다.
- 점수가 5점인 경우, 0으로 중립을 지킨다.
- 점수가 1점인 경우, -1로 적극 반대한다.

### 장점
- 값의 방향(증가 / 감소)을 표현할 수 있다
- 중심이 0이라서 균형 잡힌 표현에 유리하다

📌 단점
- Sigmoid와 마찬가지로  
  → 값이 커지면 학습이 둔해진다

---

## 6️⃣ Softmax (소프트맥스)


### 한 문장 요약
**Softmax는 여러 후보중 1등을 뽑는 선거관리위원회 함수**

### 소프트맥스는 반드시 비교 대상이 있어야 한다.
- 소프트맥스는 여러 뉴런들에게 적용되는 함수다. 
- 100%를 후보들의 점수에 따라 출력 값을 나눠준다.
- 점수 차이가 조금만 나도 결과 차이를 확 벌려버리는 특성이 있다. 

### 사용 예
비가 오는데 뭘 챙겨야 할까?
- 상황:
  - 후보 뉴런 1: 우산 (점수: 10점)
  - 후보 뉴런 2: 우비 (점수: 5점) - 비는 막지만 입고 벗기 귀찮음
  - 후보 뉴런 3: 신문지 (점수 : 1점) - 비를 거의 못 막음

- 결과값:
  - 우산: 99% (압도적 1등)
  - 우비: 0.9%
  - 신문지: 0.1%

### 언제 사용하나?
- 여러 선택지 중 하나를 고르는 문제
- 분류 문제의 **마지막 단계**

---

## 🧠 Day 2 핵심 요약

| 용어 | 핵심 의미 |
|---|---|
| Neuron | 숫자를 받아 하나의 숫자를 만드는 계산 단위 |
| Activation Function | 결과를 쓸지 말지 정하는 규칙 |
| Sigmoid | 0~1로 압축 (예/아니오 판단) |
| ReLU | 음수 제거, 양수 통과 (가장 많이 사용) |
| Tanh | -1~1로 압축 (방향성 표현) |
| Softmax | 여러 결과를 확률처럼 변환 |

---

➡️ **다음 Day 3 예고**  
> “이 계산기들이 어떻게 틀리면서도 점점 똑똑해지는가?”  
> **오차, 손실, 그리고 고치는 방법**으로 넘어간다.
